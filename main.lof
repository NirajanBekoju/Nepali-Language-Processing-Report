\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Nepali Word Cloud}}{10}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Nepali Text Generation}}{11}%
\contentsline {figure}{\numberline {1.3}{\ignorespaces Nepali Spelling Correction based on contextual meaning}}{11}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Direct Architecture of Probabilistic Language Model}}{13}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces The Transformer - model architecture}}{14}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Scaled Dot-Product Attention}}{15}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Multi-Head Attention consists of attention layer running in paralle}}{16}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Word2Vec}}{20}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces CBOW Model}}{22}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Glove Model}}{23}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Language Model for Auto-complete}}{25}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Recurrent Neural Networks}}{35}%
\contentsline {figure}{\numberline {6.2}{\ignorespaces An unrolled recurrent neural networks}}{35}%
\contentsline {figure}{\numberline {6.3}{\ignorespaces Short Sequence in RNNs}}{36}%
\contentsline {figure}{\numberline {6.4}{\ignorespaces Long Sequence in RNNs}}{36}%
\contentsline {figure}{\numberline {6.5}{\ignorespaces The repeating module in a standard RNN contains a single layer.}}{37}%
\contentsline {figure}{\numberline {6.6}{\ignorespaces The repeating module in an LSTM contains four interacting layers.}}{38}%
\contentsline {figure}{\numberline {6.7}{\ignorespaces LSTM Cell}}{38}%
\contentsline {figure}{\numberline {6.8}{\ignorespaces Bidirectional RNN}}{41}%
